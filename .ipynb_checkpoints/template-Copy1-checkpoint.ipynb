{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: load datasets\n",
    "Below we generate some toy datasets using generate_toy_datasets() as defined in utils.py. User can load their own survival datasets into \"datasets\", which should be a list of (X, time, event) tuples, where X, time, and event are the design matrix, survival time and event vectors for a given dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "\n",
    "data = scipy.io.loadmat('E:/R projects/metaCox/surv_breast_os_raw.mat')\n",
    "datasets = []\n",
    "num_datasets = len(data['dat'][0][0])\n",
    "for k in range(num_datasets):\n",
    "    datasets.append((data['dat'][0][0][k], data['days'][0][0][k].ravel(), data['status'][0][0][k].ravel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 (optional): feature transformation\n",
    "If necessary, we can first preprocess X so that it is standardized. We provide in preprocessing.py two basic types of feature transformation functions:\n",
    "- __rank_transform()__: transform features of each sample into normalized ranks\n",
    "- __zscore_transform()__: transform each feature to be zero mean and unit std across samples\n",
    "\n",
    "We can then wrap them in a FeatureTransformer object which defines the fit_transform method for our \"datasets\" list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from preprocessing import rank_transform, FeatureTransformer\n",
    "\n",
    "feature_transformer = FeatureTransformer(rank_transform)\n",
    "datasets_transformed = feature_transformer.fit_transform(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: split into training and validation\n",
    "We provide train_test_split() in utils.py to split \"datasets\" list in a stratified way. That is, each dataset in \"datasets\" is split according to test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasets_train = datasets_transformed[:2]+datasets_transformed[3:6]\n",
    "datasets_val = datasets_transformed[2:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 (optional): feature selection\n",
    "We can additionally perform a feature selection step to reduce the number of features before model training. In feature_selection.py we provide a feature selection method based on concordance index as commonly used to characterize the feature's correlation with survival. \n",
    "\n",
    "Also note that our feature selection for multiple datasets is based on meta-analysis. The concordance index is calculated for each dataset and combined into a meta-score based on the size of the dataset. This is done by wrapping the score function in a SelectKBestMeta object which defines the fit and transform function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from feature_selection import concordance_index, SelectKBestMeta\n",
    "\n",
    "topK = 1024 # select top 1024 features\n",
    "feature_selector = SelectKBestMeta(concordance_index, topK)\n",
    "feature_selector.fit(datasets_train)\n",
    "datasets_train_new = feature_selector.transform(datasets_train)\n",
    "datasets_val_new = feature_selector.transform(datasets_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: user defined keras model\n",
    "This is the core input required of the user. Below we provide a simple fully-connected network with 3 hidden layers. Note that there is no need to apply any activation function to the input layer. We are building a survival regression model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cuiyi\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Activation, Dropout\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "def build_model(feature_dim):\n",
    "    '''\n",
    "    Define a callable keras model yourself\n",
    "    model input should be a (None, feature_dim) tensor,\n",
    "    model output should be a (None, 1) tensor\n",
    "    '''\n",
    "    x = Input(shape=(feature_dim,))\n",
    "    #--------START OF USER CODE-------\n",
    "    a0 = Dropout(0.3)(x)\n",
    "    z1 = Dense(units=1024, activation=None)(a0)\n",
    "    a1 = Activation(activation='elu')(z1)\n",
    "    a1 = Dropout(0.5)(a1)\n",
    "    z2 = Dense(units=1024, activation=None)(a1)\n",
    "    a2 = Activation(activation='elu')(z2)\n",
    "    a2 = Dropout(0.5)(a2)\n",
    "    y = Dense(units=1, activation=None)(a2)\n",
    "    #--------END OF USER CODE-------\n",
    "    \n",
    "    model = Model(inputs=x, outputs=y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: create a model and train\n",
    "We provide a high-level class SurvivalModel to facilitate model training. In SurvivalModel.fit(), There are two modes for model training: merge or decentralized. For mode='dencentral'. each dataset will be treated as a mini-batch. For mode='merge', the datasets are merged into a single dataset and mini-batches are sampled from the merged dataset. If your datasets are very heterogenous (eg different cancers), consider mode='decentral'; otherwise, mode='merge' should be the choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cuiyi\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss_train=0.6609, ci_train=0.7106, loss_val=0.8288, ci_val=0.6390\n",
      "Epoch 100: loss_train=0.5413, ci_train=0.7794, loss_val=0.7687, ci_val=0.6438\n",
      "Epoch 200: loss_train=0.4945, ci_train=0.7991, loss_val=0.7688, ci_val=0.6449\n",
      "Epoch 300: loss_train=0.4524, ci_train=0.8147, loss_val=0.7607, ci_val=0.6514\n",
      "Epoch 400: loss_train=0.4175, ci_train=0.8312, loss_val=0.7287, ci_val=0.6696\n",
      "Epoch 500: loss_train=0.3989, ci_train=0.8445, loss_val=0.7250, ci_val=0.6769\n",
      "Epoch 600: loss_train=0.3609, ci_train=0.8591, loss_val=0.7102, ci_val=0.6902\n",
      "Epoch 700: loss_train=0.3648, ci_train=0.8598, loss_val=0.7084, ci_val=0.6953\n",
      "Epoch 800: loss_train=0.3012, ci_train=0.8851, loss_val=0.6887, ci_val=0.7016\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-3a084396f0ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msurvival_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSurvivalModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0msurvival_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatasets_train_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatasets_val_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'logloss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'decentral'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\cuiyi\\Documents\\python_projects\\deep_survival\\DeepSurvival\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, datasets_train, datasets_val, loss_func, epochs, lr, mode, batch_size)\u001b[0m\n\u001b[0;32m     43\u001b[0m                   \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_merge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'decentral'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m                   \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_decentral\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cuiyi\\Documents\\python_projects\\deep_survival\\DeepSurvival\\models.py\u001b[0m in \u001b[0;36m_train_decentral\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    133\u001b[0m                                                                                           \u001b[0midx1\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0midx1_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m                                                                                           \u001b[0midx2\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0midx2_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m \t\t\t\t\t\t\t\t\t\t\t  K.learning_phase(): 1})\n\u001b[0m\u001b[0;32m    136\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                                 \u001b[0mloss_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mci_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_total_loss_ci\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_pairs_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cuiyi\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cuiyi\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cuiyi\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cuiyi\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cuiyi\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from models import SurvivalModel\n",
    "\n",
    "survival_model = SurvivalModel(build_model)\n",
    "survival_model.fit(datasets_train_new, datasets_val_new, loss_func='logloss', epochs=2000, lr=0.001, mode='decentral')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model achieves an almost perfect performance on the training dataset but not so on the testing dataset. This is expected since our simulated datasets are just randomly generated and there is nothing to learn (it'll be surprising if it does learn anything useful...). You can provide your own dataset and design your own model and check if it also works on testing dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: predict on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = survival_model.predict(datasets_val_new[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cindex, _ = concordance_index(y_pred, datasets_val_new[0][1], datasets_val_new[0][2])\n",
    "print(cindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
