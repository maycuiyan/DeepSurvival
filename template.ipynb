{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: load datasets\n",
    "Below we generate some toy datasets using generate_toy_datasets() as defined in utils.py. User can load their own survival datasets into \"datasets\", which should be a list of (X, time, event) tuples, where X, time, and event are the design matrix, survival time and event vectors for a given dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import generate_toy_datasets\n",
    "\n",
    "n_datasets = 10 # generate 10 datasets\n",
    "n_min, n_max = 100, 200 # number of samples in each dataset is a random integer between 100 and 200\n",
    "n_features = 10000 # number of features is 10000\n",
    "datasets = generate_toy_datasets(n_datasets, n_min, n_max, n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 (optional): feature transformation\n",
    "If necessary, we can first preprocess X so that it is standardized. We provide in preprocessing.py two basic types of feature transformation functions:\n",
    "- __rank_transform()__: transform features of each sample into normalized ranks\n",
    "- __zscore_transform()__: transform each feature to be zero mean and unit std across samples\n",
    "\n",
    "We can then wrap them in a FeatureTransformer object which defines the fit_transform method for our \"datasets\" list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from preprocessing import rank_transform, FeatureTransformer\n",
    "\n",
    "feature_transformer = FeatureTransformer(rank_transform)\n",
    "datasets_transformed = feature_transformer.fit_transform(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: split into training and validation\n",
    "We provide train_test_split() in utils.py to split \"datasets\" list in a stratified way. That is, each dataset in \"datasets\" is split according to test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import train_test_split\n",
    "\n",
    "datasets_train, datasets_val = train_test_split(datasets_transformed, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 (optional): feature selection\n",
    "We can additionally perform a feature selection step to reduce the number of features before model training. In feature_selection.py we provide a feature selection method based on concordance index as commonly used to characterize the feature's correlation with survival. \n",
    "\n",
    "Also note that our feature selection for multiple datasets is based on meta-analysis. The concordance index is calculated for each dataset and combined into a meta-score based on the size of the dataset. This is done by wrapping the score function in a SelectKBestMeta object which defines the fit and transform function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from feature_selection import concordance_index, SelectKBestMeta\n",
    "\n",
    "topK = 1024 # select top 1024 features\n",
    "\n",
    "## fit on training\n",
    "feature_selector = SelectKBestMeta(concordance_index, topK)\n",
    "feature_selector.fit(datasets_train)\n",
    "\n",
    "## apply to both\n",
    "datasets_train_new = feature_selector.transform(datasets_train)\n",
    "datasets_val_new = feature_selector.transform(datasets_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: user defined keras model\n",
    "This is the core input required of the user. Below we provide a simple fully-connected network with 3 hidden layers. Note that there is no need to apply any activation function to the input layer. We are building a survival regression model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Activation, Dropout\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "def model_builder(input_shape):\n",
    "    '''\n",
    "    Define a callable keras model yourself\n",
    "    '''\n",
    "    x = Input(shape=input_shape)\n",
    "    #--------START OF USER CODE-------\n",
    "    a0 = Dropout(0.3)(x)\n",
    "    z1 = Dense(units=1024, activation=None)(a0)\n",
    "    a1 = Activation(activation='elu')(z1)\n",
    "    a1 = Dropout(0.5)(a1)\n",
    "    z2 = Dense(units=1024, activation=None)(a1)\n",
    "    a2 = Activation(activation='elu')(z2)\n",
    "    a2 = Dropout(0.5)(a2)\n",
    "    y = Dense(units=1, activation=None)(a2)\n",
    "    #--------END OF USER CODE-------\n",
    "    \n",
    "    model = Model(inputs=x, outputs=y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: create a model and train\n",
    "We provide a high-level class SurvivalModel to facilitate model training. In SurvivalModel.fit(), There are two modes for model training: merge or decentralized. For mode='dencentral'. each dataset will be treated as a mini-batch. For mode='merge', the datasets are merged into a single dataset and mini-batches are sampled from the merged dataset. If your datasets are very heterogenous (eg different cancers), consider mode='decentral'; otherwise, mode='merge' should be the choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from models import SurvivalModel\n",
    "\n",
    "survival_model = SurvivalModel(model_builder)\n",
    "survival_model.fit(datasets_train_new, datasets_val_new, loss_func='cox', epochs=1000, lr=0.001, mode='decentralize')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model achieves an almost perfect performance on the training dataset but not so on the testing dataset. This is expected since our simulated datasets are just randomly generated and there is nothing to learn (it'll be surprising if it does learn anything useful...). You can provide your own dataset and check if it also works on testing dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: predict on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, time_test, event_test = datasets_train_new[2]\n",
    "score_test = survival_model.predict(X_test).ravel()\n",
    "_, cindex = survival_model.evaluate(X_test, time_test, event_test)\n",
    "print(cindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
