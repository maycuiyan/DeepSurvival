{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: load datasets\n",
    "Below we generate some toy datasets using generate_toy_datasets() as defined in utils.py. User can load their own survival datasets into \"datasets\", which should be a list of (X, time, event) tuples, where X, time, and event are the design matrix, survival time and event vectors for a given dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import generate_toy_datasets\n",
    "\n",
    "n_datasets = 10 # generate 10 datasets\n",
    "n_min, n_max = 100, 200 # number of samples in each dataset is a random integer between 100 and 200\n",
    "n_features = 10000 # number of features is 10000\n",
    "datasets = generate_toy_datasets(n_datasets, n_min, n_max, n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 (optional): feature transformation\n",
    "If necessary, we can first preprocess X so that it is standardized. We provide in preprocessing.py two basic types of feature transformation functions:\n",
    "- __rank_transform()__: transform features of each sample into normalized ranks\n",
    "- __zscore_transform()__: transform each feature to be zero mean and unit std across samples\n",
    "\n",
    "We can then wrap them in a FeatureTransformer object which defines the fit_transform method for our \"datasets\" list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from preprocessing import rank_transform, FeatureTransformer\n",
    "\n",
    "feature_transformer = FeatureTransformer(rank_transform)\n",
    "datasets_transformed = feature_transformer.fit_transform(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: split into training and testing\n",
    "We provide train_test_split() in utils.py to split \"datasets\" list in a stratified way. That is, each dataset in \"datasets\" is split according to test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import train_test_split\n",
    "\n",
    "datasets_train, datasets_test = train_test_split(datasets_transformed, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: precompute index pairs\n",
    "For each dataset in the datasets list, we precompute a (idx1, idx2) tuple, where idx1 and idx2 are two equal length index vectors satisfying time[idx1] < time[idx2] and event[idx1] == 0. This information is needed when defining the loss function for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import get_index_pairs\n",
    "\n",
    "index_pairs_tr = get_index_pairs(datasets_train)\n",
    "index_pairs_te = get_index_pairs(datasets_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 (optional): feature selection\n",
    "We can additionally perform a feature selection step to reduce the number of features before model training. In feature_selection.py we provide a feature selection method based on concordance index as commonly used to characterize the feature's correlation with survival. \n",
    "\n",
    "Also note that our feature selection for multiple datasets is based on meta-analysis. The concordance index is calculated for each dataset and combined into a meta-score based on the size of the dataset. This is done by wrapping the score function in a SelectKBestMeta object which defines the fit and transform function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from feature_selection import concordance_index, SelectKBestMeta\n",
    "\n",
    "topK = 1024 # select top 1024 features\n",
    "feature_selector = SelectKBestMeta(concordance_index, topK)\n",
    "feature_selector.fit(datasets_train)\n",
    "datasets_train_new = feature_selector.transform(datasets_train)\n",
    "datasets_test_new = feature_selector.transform(datasets_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: user defined keras model\n",
    "This is the core input required of the user. Below we provide a simple fully-connected network with 3 hidden layers. Note that there is no need to apply any activation function to the input layer. We are building a survival regression model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Activation, Dropout\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "def build_model(feature_dim):\n",
    "    '''\n",
    "    Define a callable keras model yourself\n",
    "    model input should be a (None, feature_dim) tensor,\n",
    "    model output should be a (None, 1) tensor\n",
    "    '''\n",
    "    x = Input(shape=(feature_dim,))\n",
    "    #--------START OF USER CODE-------\n",
    "    a0 = Dropout(0.3)(x)\n",
    "    z1 = Dense(units=1024, activation=None)(a0)\n",
    "    a1 = Activation(activation='elu')(z1)\n",
    "    a1 = Dropout(0.5)(a1)\n",
    "    z2 = Dense(units=1024, activation=None)(a1)\n",
    "    a2 = Activation(activation='elu')(z2)\n",
    "    a2 = Dropout(0.5)(a2)\n",
    "    y = Dense(units=1, activation=None)(a2)\n",
    "    #--------END OF USER CODE-------\n",
    "    \n",
    "    model = Model(inputs=x, outputs=y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: build tensorflow graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cuiyi\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.name_scope('input'):\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=[None, topK], name='X')\n",
    "    idx1 = tf.placeholder(dtype=tf.int32, shape=[None, ])\n",
    "    idx2 = tf.placeholder(dtype=tf.int32, shape=[None, ])\n",
    "    \n",
    "with tf.name_scope('model'):\n",
    "    model = build_model(topK)\n",
    "    \n",
    "with tf.name_scope('output'):\n",
    "    y_pred = model(X)\n",
    "    y1 = tf.gather(y_pred, idx1)\n",
    "    y2 = tf.gather(y_pred, idx2)\n",
    "    \n",
    "with tf.name_scope('metrics'):\n",
    "    loss = tf.reduce_mean(tf.maximum(1+y1-y2, 0.0)) # alternatively: loss = tf.reduce_mean(tf.log(1+tf.exp(y1-y2)))\n",
    "    ci = tf.reduce_mean(tf.cast(y1<y2, tf.float32), name='c_index')\n",
    "    \n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "    train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: run model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss_tr=0.3691, ci_tr=0.8500, loss_te=1.1415, ci_te=0.4619\n",
      "Epoch 100: loss_tr=0.1482, ci_tr=0.9556, loss_te=1.0740, ci_te=0.4899\n",
      "Epoch 200: loss_tr=0.1154, ci_tr=0.9634, loss_te=1.1546, ci_te=0.4838\n",
      "Epoch 300: loss_tr=0.1026, ci_tr=0.9734, loss_te=1.1217, ci_te=0.4742\n",
      "Epoch 400: loss_tr=0.0716, ci_tr=0.9818, loss_te=1.1532, ci_te=0.4834\n",
      "Epoch 500: loss_tr=0.0585, ci_tr=0.9861, loss_te=1.1448, ci_te=0.4869\n",
      "Epoch 600: loss_tr=0.0524, ci_tr=0.9881, loss_te=1.1674, ci_te=0.4829\n",
      "Epoch 700: loss_tr=0.0501, ci_tr=0.9905, loss_te=1.1148, ci_te=0.4921\n",
      "Epoch 800: loss_tr=0.0444, ci_tr=0.9927, loss_te=1.1157, ci_te=0.4764\n",
      "Epoch 900: loss_tr=0.0338, ci_tr=0.9944, loss_te=1.1923, ci_te=0.4663\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "epochs = 1000\n",
    "sess = tf.Session()\n",
    "\n",
    "## a function just to make model eval eaiser\n",
    "def total_loss_ci(datasets, index_pairs):\n",
    "    loss_total = 0\n",
    "    ci_total = 0\n",
    "    n_total = 0\n",
    "    for i, (X_batch, _, _) in enumerate(datasets):\n",
    "        idx1_batch, idx2_batch = index_pairs[i]\n",
    "        loss_batch, ci_batch = sess.run([loss, ci], feed_dict={X:X_batch, idx1:idx1_batch, idx2:idx2_batch, K.learning_phase():0})\n",
    "        loss_total += len(idx1_batch)*loss_batch\n",
    "        ci_total += len(idx1_batch)*ci_batch\n",
    "        n_total += len(idx1_batch)\n",
    "    loss_total /= n_total\n",
    "    ci_total /= n_total\n",
    "    return loss_total, ci_total\n",
    "\n",
    "sess.run(init)\n",
    "for epoch in range(epochs):\n",
    "    for i, (X_batch, _, _) in enumerate(datasets_train_new):\n",
    "        idx1_batch, idx2_batch = index_pairs_tr[i]\n",
    "        sess.run(train_op, feed_dict={X:X_batch, idx1:idx1_batch, idx2:idx2_batch, K.learning_phase():1})\n",
    "    if epoch%100==0:\n",
    "        loss_tr, ci_tr = total_loss_ci(datasets_train_new, index_pairs_tr)\n",
    "        loss_te, ci_te = total_loss_ci(datasets_test_new, index_pairs_te)\n",
    "        print('Epoch {0}: loss_tr={1:5.4f}, ci_tr={2:5.4f}, loss_te={3:5.4f}, ci_te={4:5.4f}'.format(epoch, loss_tr, ci_tr, loss_te, ci_te))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model achieves an almost perfect performance on the training dataset but not so on the testing dataset. This is expected since our simulated datasets are just randomly generated and there is nothing to learn (it'll be surprising if it does learn anything useful...). You can provide your own dataset and design your own model and check if it also works on testing dataset. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
